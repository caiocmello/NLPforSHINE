{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with loading all necessary libraries\n",
    "\n",
    "import spacy\n",
    "import os\n",
    "import csv\n",
    "from itertools import zip_longest\n",
    "import nltk\n",
    "import wordcloud\n",
    "import json\n",
    "from spacy.lang.en.stop_words import STOP_WORDS as en_stopwords\n",
    "from spacy.lang.pt.stop_words import STOP_WORDS as pt_stopwords\n",
    "from spacy.lang.es.stop_words import STOP_WORDS as es_stopwords\n",
    "import sys\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import ngrams\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "import numpy as np\n",
    "from os import path\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stopwords(lang):\n",
    "    if lang == 'english':\n",
    "        df_stopwords = set(en_stopwords)\n",
    "        custom_stopwords = set(['olympics', 'olympic', 'london', 'person',\n",
    "                                'rio', 'legacy', '2012', '2016',\n",
    "                                'said', 'caption', 'image', 'years',\n",
    "                                'd','s','t','m','n','ve', 'll', 'xe2', 'x80', 'x99', 'x99s', 'nthe', 'lda', '000', 'xc2'])\n",
    "        return df_stopwords | custom_stopwords\n",
    "    elif lang == 'portuguese':\n",
    "        df_stopwords = set(pt_stopwords)\n",
    "        custom_stopwords = set(['legado', 'olimpico', 'london',\n",
    "                                'olímpica', 'londres', 'rio', 'legado',\n",
    "                                'olímpico', '2012', '2016'])\n",
    "        return df_stopwords | custom_stopwords\n",
    "    \n",
    "    \n",
    "def my_tokenizers(doc):\n",
    "    words = nltk.word_tokenize(doc)\n",
    "    return [word for word in words if len(word) > 2]\n",
    "\n",
    "def get_top_n_words(corpus, stopwords, n=20):\n",
    "    vec = CountVectorizer(stop_words = stopwords).fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0)\n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    return words_freq[:n]\n",
    "\n",
    "def get_top_n_bigram(corpus, stopwords, n=20):\n",
    "    vec = CountVectorizer(ngram_range=(2, 2), stop_words = stopwords).fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0)\n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    return words_freq[:n]\n",
    "\n",
    "def get_top_n_words_tfidf(corpus, stopwords, n=20):\n",
    "    vec = TfidfVectorizer(stop_words = stopwords).fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0)\n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    return words_freq[:n]\n",
    "\n",
    "def get_top_n_bigram_tfidf(corpus, stopwords, n=20):\n",
    "    vec = TfidfVectorizer(ngram_range=(2, 2), stop_words = stopwords).fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0)\n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    return words_freq[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain = 'bbc' #Try also 'bbc','businessinsider','buzzfeed','citymonitor','gamesmonitor','guardian','independentgoogle','itv','metro','queenelizabetholympicpark','sky','sun','theconversation','theindependent','uksport','g-bbc','g-dailymail','g-telegraph','g-guardian'\n",
    "lang = 'english'\n",
    "entitieslocal = './data/'+ domain + '/data_analysis/all_texts_together' + domain + '.txt' \n",
    "entitieslocal2 = './data/'+ domain + '/data_analysis/' \n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(entitieslocal), 'r', encoding='utf-8') as myfile: \n",
    "            documento = myfile.read()\n",
    "            doc = nlp(documento)\n",
    "                   \n",
    "\n",
    "# document level\n",
    "ents = [(e.label_, e.text) for e in doc.ents if not e.text.isspace()]\n",
    "print(ents)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------ WRITE CSV WITH DATA ----------------#\n",
    "\n",
    "entity = [(e.label_) for e in doc.ents if not e.text.isspace()]\n",
    "word = [(e.text) for e in doc.ents if not e.text.isspace()]\n",
    "\n",
    "d = [entity,word]\n",
    "export_data = zip_longest(*d, fillvalue = '')\n",
    "with open(entitieslocal2 + 'entexcel.csv', 'w', newline='') as myfile:\n",
    "      wr = csv.writer(myfile)\n",
    "      wr.writerow((\"Entity\", \"Text\"))\n",
    "      wr.writerows(export_data)\n",
    "myfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------ WRITE results in TXT ----------------#\n",
    "\n",
    "Keyword = 'PERSON' # Try also PERSON, FAC, GPE, LOC, MONEY, ORG\n",
    "list = []\n",
    "with open(entitieslocal2 + 'entexcel.csv', 'r') as f:\n",
    "    for row in csv.reader(f):\n",
    "        try:\n",
    "            if Keyword in row:\n",
    "                list.append(row)\n",
    "                #print(row)\n",
    "                \n",
    "                #writer.writerow(row)\n",
    "        except:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(entitieslocal2 + Keyword + '.txt'), 'w+') as file1:\n",
    "    file1.write(str(list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------ Analyse results with NLTK ----------------#\n",
    "\n",
    "with open(os.path.join(entitieslocal2 + Keyword + '.txt'), 'r') as file2: \n",
    "            docENT = file2.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TOP N WORDS FOR ENTITIES\n",
    "get_top_n_words([docENT], get_stopwords(lang), 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_top_n_bigram([docENT], get_stopwords(lang), 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------ WORDCLOUD ----------------#\n",
    "\n",
    "text = docENT\n",
    "# Create stopword list:\n",
    "stopwords = set(STOPWORDS)\n",
    "stopwords.update(['olympics', 'PERSON', \"\"\"PERSON'\"\"\", \"\"\"Olympic'\"\"\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(stopwords=stopwords, max_words=100, background_color=\"white\", width=1500, height=800).generate(text)\n",
    "#plt.figure()\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the image in the img folder:\n",
    "wordcloud.to_file('./data_analysis/' + domain + Keyword +'.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------ ENTIRE DATA ANALYSIS-----------------------------------------------------------#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory1 = 'bbc'\n",
    "directory2 = 'businessinsider'\n",
    "directory3 = 'buzzfeed'\n",
    "directory4 = 'citymonitor'\n",
    "directory5 = 'gamesmonitor'\n",
    "directory6 = 'guardian'\n",
    "directory7 = 'independentgoogle'\n",
    "directory8 = 'itv'\n",
    "directory9 = 'metro'\n",
    "directory10 = 'queenelizabetholympicpark'\n",
    "directory11 = 'sky'\n",
    "directory12 = 'sun'\n",
    "directory13 = 'theconversation'\n",
    "directory14 = 'theindependent'\n",
    "directory15 = 'uksport'\n",
    "directory16 = 'g-bbc'\n",
    "directory17 = 'g-dailymail'\n",
    "directory18 = 'g-telegraph'\n",
    "directory19 = 'g-guardian'\n",
    "\n",
    "\n",
    "localolympic1 = './data/' + directory1 + '/data_analysis/'\n",
    "localolympic2 = './data/' + directory2 + '/data_analysis/'\n",
    "localolympic3 = './data/' + directory3 + '/data_analysis/'\n",
    "localolympic4 = './data/' + directory4 + '/data_analysis/'\n",
    "localolympic5 = './data/' + directory5 + '/data_analysis/'\n",
    "localolympic6 = './data/' + directory6 + '/data_analysis/'\n",
    "localolympic7 = './data/' + directory7 + '/data_analysis/'\n",
    "localolympic8 = './data/' + directory8 + '/data_analysis/'\n",
    "localolympic9 = './data/' + directory9 + '/data_analysis/'\n",
    "localolympic10 = './data/' + directory10 + '/data_analysis/'\n",
    "localolympic11 = './data/' + directory11 + '/data_analysis/'\n",
    "localolympic12 = './data/' + directory12 + '/data_analysis/'\n",
    "localolympic13 = './data/' + directory13 + '/data_analysis/'\n",
    "localolympic14 = './data/' + directory14 + '/data_analysis/'\n",
    "localolympic15 = './data/' + directory15 + '/data_analysis/'\n",
    "localolympic16 = './data/' + directory16 + '/data_analysis/'\n",
    "localolympic17 = './data/' + directory17 + '/data_analysis/'\n",
    "localolympic18 = './data/' + directory18 + '/data_analysis/'\n",
    "localolympic19 = './data/' + directory19 + '/data_analysis/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(localolympic1 + 'all_texts_together' + directory1 + '.txt'), 'r', encoding= 'utf8') as myfile1:\n",
    "    content1 = myfile1.read()\n",
    "with open(os.path.join(localolympic2 + 'all_texts_together' + directory2 + '.txt'), 'r', encoding= 'utf8') as myfile2:\n",
    "    content2 = myfile2.read()\n",
    "with open(os.path.join(localolympic3 + 'all_texts_together' + directory3 + '.txt'), 'r', encoding= 'utf8') as myfile3:\n",
    "    content3 = myfile3.read()\n",
    "with open(os.path.join(localolympic4 + 'all_texts_together' + directory4 + '.txt'), 'r', encoding= 'utf8') as myfile4:\n",
    "    content4 = myfile4.read()\n",
    "with open(os.path.join(localolympic5 + 'all_texts_together' + directory5 + '.txt'), 'r', encoding= 'utf8') as myfile5:\n",
    "    content5 = myfile5.read()\n",
    "with open(os.path.join(localolympic6 + 'all_texts_together' + directory6 + '.txt'), 'r', encoding= 'utf8') as myfile6:\n",
    "    content6 = myfile6.read()\n",
    "with open(os.path.join(localolympic7 + 'all_texts_together' + directory7 + '.txt'), 'r', encoding= 'utf8') as myfile7:\n",
    "    content7 = myfile7.read()\n",
    "with open(os.path.join(localolympic8 + 'all_texts_together' + directory8 + '.txt'), 'r', encoding= 'utf8') as myfile8:\n",
    "    content8 = myfile8.read()\n",
    "with open(os.path.join(localolympic9 + 'all_texts_together' + directory9 + '.txt'), 'r', encoding= 'utf8') as myfile9:\n",
    "    content9 = myfile9.read()\n",
    "with open(os.path.join(localolympic10 + 'all_texts_together' + directory10 + '.txt'), 'r', encoding= 'utf8') as myfile10:\n",
    "    content10 = myfile10.read()\n",
    "with open(os.path.join(localolympic11 + 'all_texts_together' + directory11 + '.txt'), 'r', encoding= 'utf8') as myfile11:\n",
    "    content11 = myfile11.read()\n",
    "with open(os.path.join(localolympic12 + 'all_texts_together' + directory12 + '.txt'), 'r', encoding= 'utf8') as myfile12:\n",
    "    content12 = myfile12.read()\n",
    "with open(os.path.join(localolympic13 + 'all_texts_together' + directory13 + '.txt'), 'r', encoding= 'utf8') as myfile13:\n",
    "    content13 = myfile13.read()\n",
    "with open(os.path.join(localolympic14 + 'all_texts_together' + directory14 + '.txt'), 'r', encoding= 'utf8') as myfile14:\n",
    "    content14 = myfile14.read()\n",
    "with open(os.path.join(localolympic15 + 'all_texts_together' + directory15 + '.txt'), 'r', encoding= 'utf8') as myfile15:\n",
    "    content15 = myfile15.read()\n",
    "with open(os.path.join(localolympic16 + 'all_texts_together' + directory16 + '.txt'), 'r', encoding= 'utf8') as myfile16:\n",
    "    content16 = myfile16.read()\n",
    "with open(os.path.join(localolympic17 + 'all_texts_together' + directory17 + '.txt'), 'r', encoding= 'utf8') as myfile17:\n",
    "    content17 = myfile17.read()\n",
    "with open(os.path.join(localolympic18 + 'all_texts_together' + directory18 + '.txt'), 'r', encoding= 'utf8') as myfile18:\n",
    "    content18 = myfile18.read()\n",
    "with open(os.path.join(localolympic19 + 'all_texts_together' + directory19 + '.txt'), 'r', encoding= 'utf8') as myfile19:\n",
    "    content19 = myfile19.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.max_length = 11000000 \n",
    "doc2 = nlp(content1+content2+content3+content4+content5+content6+content7+content8+content9+content10+content11+content12+content13+content14+content15+content16+content17+content18+content19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
